{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f0ac3df-7ea4-49eb-870f-a273e451671e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from create_data import DumbCirc as dc\n",
    "from dklsuq import DeepKernelSUQ as dklu\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af8a33da-7250-4812-b735-282b95c9901e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_instance = dc(0.5, 0.5, 0.25)\n",
    "fc, pc, tc = data_instance.create_dataset(4, 2, [50], [0.2], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4efc306-cfad-424d-befe-7d3a298102f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_202240/2979240048.py:2: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /opt/conda/conda-bld/pytorch_1711403246168/work/torch/csrc/utils/tensor_new.cpp:275.)\n",
      "  fc, pc, tc = torch.tensor(fc), torch.tensor(pc), torch.tensor(tc)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "fc, pc, tc = torch.tensor(fc), torch.tensor(pc), torch.tensor(tc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ddc3109-a660-4743-a97a-59b64359cdbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 140, 2])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fc.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a7a5eea-4821-4bb1-8e8f-895005d5f9b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 35, 2])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pc.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b9ec20f-6db0-460f-840b-8000b26727c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = dklu(point_cloud=fc, partial_cloud=pc, test_partial=tc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6193567b-fbc7-48d1-b861-ba8591f47045",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "model.set_device(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1fb298cb-5616-420c-8a0e-c7ea0b01dc81",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.set_test_data(tc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bdfaf80e-4092-4f42-a202-a6535fde2e86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/servers/ash/hdd-home3/dghosh/Documents/Thesis/1notebooks/dklsuq.py:123: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at /opt/conda/conda-bld/pytorch_1711403246168/work/aten/src/ATen/native/TensorShape.cpp:3637.)\n",
      "  + posterior_mean.T @ torch.linalg.inv(posterior_diag) @ posterior_mean)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 0.453551\n",
      "Epoch: 1, Loss: 0.466374\n",
      "Epoch: 2, Loss: 0.441700\n",
      "Epoch: 3, Loss: 0.453651\n",
      "Epoch: 4, Loss: 0.447047\n",
      "Epoch: 5, Loss: 0.479399\n",
      "Epoch: 6, Loss: 0.464010\n",
      "Epoch: 7, Loss: 0.448291\n",
      "Epoch: 8, Loss: 0.439021\n",
      "Epoch: 9, Loss: 0.438762\n",
      "Epoch: 10, Loss: 0.473526\n",
      "Epoch: 11, Loss: 0.459033\n",
      "Epoch: 12, Loss: 0.435276\n",
      "Epoch: 13, Loss: 0.436419\n",
      "Epoch: 14, Loss: 0.442996\n",
      "Epoch: 15, Loss: 0.432849\n",
      "Epoch: 16, Loss: 0.427302\n",
      "Epoch: 17, Loss: 0.440488\n",
      "Epoch: 18, Loss: 0.420458\n",
      "Epoch: 19, Loss: 0.423267\n",
      "Epoch: 20, Loss: 0.434703\n",
      "Epoch: 21, Loss: 0.427574\n",
      "Epoch: 22, Loss: 0.432330\n",
      "Epoch: 23, Loss: 0.432710\n",
      "Epoch: 24, Loss: 0.407462\n",
      "Epoch: 25, Loss: 0.396658\n",
      "Epoch: 26, Loss: 0.379561\n",
      "Epoch: 27, Loss: 0.399515\n",
      "Epoch: 28, Loss: 0.394682\n",
      "Epoch: 29, Loss: 0.400411\n",
      "Epoch: 30, Loss: 0.405061\n",
      "Epoch: 31, Loss: 0.405520\n",
      "Epoch: 32, Loss: 0.399246\n",
      "Epoch: 33, Loss: 0.409848\n",
      "Epoch: 34, Loss: 0.413571\n",
      "Epoch: 35, Loss: 0.391710\n",
      "Epoch: 36, Loss: 0.385550\n",
      "Epoch: 37, Loss: 0.383741\n",
      "Epoch: 38, Loss: 0.382484\n",
      "Epoch: 39, Loss: 0.387275\n",
      "Epoch: 40, Loss: 0.378361\n",
      "Epoch: 41, Loss: 0.362841\n",
      "Epoch: 42, Loss: 0.378449\n",
      "Epoch: 43, Loss: 0.363495\n",
      "Epoch: 44, Loss: 0.357118\n",
      "Epoch: 45, Loss: 0.370349\n",
      "Epoch: 46, Loss: 0.346997\n",
      "Epoch: 47, Loss: 0.342702\n",
      "Epoch: 48, Loss: 0.347637\n",
      "Epoch: 49, Loss: 0.355812\n",
      "Epoch: 50, Loss: 0.350425\n",
      "Epoch: 51, Loss: 0.343537\n",
      "Epoch: 52, Loss: 0.342631\n",
      "Epoch: 53, Loss: 0.346932\n",
      "Epoch: 54, Loss: 0.342571\n",
      "Epoch: 55, Loss: 0.325869\n",
      "Epoch: 56, Loss: 0.329530\n",
      "Epoch: 57, Loss: 0.317665\n",
      "Epoch: 58, Loss: 0.311645\n",
      "Epoch: 59, Loss: 0.325585\n",
      "Epoch: 60, Loss: 0.334264\n",
      "Epoch: 61, Loss: 0.314167\n",
      "Epoch: 62, Loss: 0.304579\n",
      "Epoch: 63, Loss: 0.311581\n",
      "Epoch: 64, Loss: 0.310323\n",
      "Epoch: 65, Loss: 0.321964\n",
      "Epoch: 66, Loss: 0.311126\n",
      "Epoch: 67, Loss: 0.315372\n",
      "Epoch: 68, Loss: 0.315406\n",
      "Epoch: 69, Loss: 0.295001\n",
      "Epoch: 70, Loss: 0.293379\n",
      "Epoch: 71, Loss: 0.290533\n",
      "Epoch: 72, Loss: 0.290705\n",
      "Epoch: 73, Loss: 0.285023\n",
      "Epoch: 74, Loss: 0.274277\n",
      "Epoch: 75, Loss: 0.285062\n",
      "Epoch: 76, Loss: 0.283540\n",
      "Epoch: 77, Loss: 0.289750\n",
      "Epoch: 78, Loss: 0.288794\n",
      "Epoch: 79, Loss: 0.284990\n",
      "Epoch: 80, Loss: 0.284790\n",
      "Epoch: 81, Loss: 0.284979\n",
      "Epoch: 82, Loss: 0.289350\n",
      "Epoch: 83, Loss: 0.285838\n",
      "Epoch: 84, Loss: 0.284030\n",
      "Epoch: 85, Loss: 0.286989\n",
      "Epoch: 86, Loss: 0.301478\n",
      "Epoch: 87, Loss: 0.299958\n",
      "Epoch: 88, Loss: 0.299681\n",
      "Epoch: 89, Loss: 0.301483\n",
      "Epoch: 90, Loss: 0.298198\n",
      "Epoch: 91, Loss: 0.300015\n",
      "Epoch: 92, Loss: 0.293417\n",
      "Epoch: 93, Loss: 0.289220\n",
      "Epoch: 94, Loss: 0.284622\n",
      "Epoch: 95, Loss: 0.281028\n",
      "Epoch: 96, Loss: 0.282050\n",
      "Epoch: 97, Loss: 0.282367\n",
      "Epoch: 98, Loss: 0.282339\n",
      "Epoch: 99, Loss: 0.284321\n",
      "Epoch: 100, Loss: 0.280939\n",
      "Epoch: 101, Loss: 0.268604\n",
      "Epoch: 102, Loss: 0.273133\n",
      "Epoch: 103, Loss: 0.283645\n",
      "Epoch: 104, Loss: 0.287154\n",
      "Epoch: 105, Loss: 0.281366\n",
      "Epoch: 106, Loss: 0.275941\n",
      "Epoch: 107, Loss: 0.279913\n",
      "Epoch: 108, Loss: 0.296263\n",
      "Epoch: 109, Loss: 0.290218\n",
      "Epoch: 110, Loss: 0.272778\n",
      "Epoch: 111, Loss: 0.279351\n",
      "Epoch: 112, Loss: 0.277113\n",
      "Epoch: 113, Loss: 0.276477\n",
      "Epoch: 114, Loss: 0.277121\n",
      "Epoch: 115, Loss: 0.275172\n",
      "Epoch: 116, Loss: 0.267233\n",
      "Epoch: 117, Loss: 0.266278\n",
      "Epoch: 118, Loss: 0.260808\n",
      "Epoch: 119, Loss: 0.258562\n",
      "Epoch: 120, Loss: 0.256326\n",
      "Epoch: 121, Loss: 0.255865\n",
      "Epoch: 122, Loss: 0.254806\n",
      "Epoch: 123, Loss: 0.253203\n",
      "Epoch: 124, Loss: 0.255783\n",
      "Epoch: 125, Loss: 0.254866\n",
      "Epoch: 126, Loss: 0.254097\n",
      "Epoch: 127, Loss: 0.249430\n",
      "Epoch: 128, Loss: 0.244829\n",
      "Epoch: 129, Loss: 0.247608\n",
      "Epoch: 130, Loss: 0.245649\n",
      "Epoch: 131, Loss: 0.252553\n",
      "Epoch: 132, Loss: 0.239412\n",
      "Epoch: 133, Loss: 0.238018\n",
      "Epoch: 134, Loss: 0.235862\n",
      "Epoch: 135, Loss: 0.243615\n",
      "Epoch: 136, Loss: 0.238966\n",
      "Epoch: 137, Loss: 0.263347\n",
      "Epoch: 138, Loss: 0.254981\n",
      "Epoch: 139, Loss: 0.253833\n",
      "Epoch: 140, Loss: 0.251509\n",
      "Epoch: 141, Loss: 0.253286\n",
      "Epoch: 142, Loss: 0.257689\n",
      "Epoch: 143, Loss: 0.257605\n",
      "Epoch: 144, Loss: 0.256412\n",
      "Epoch: 145, Loss: 0.253464\n",
      "Epoch: 146, Loss: 0.246565\n",
      "Epoch: 147, Loss: 0.241785\n",
      "Epoch: 148, Loss: 0.246007\n",
      "Epoch: 149, Loss: 0.241033\n",
      "Epoch: 150, Loss: 0.239935\n",
      "Epoch: 151, Loss: 0.236930\n",
      "Epoch: 152, Loss: 0.241725\n",
      "Epoch: 153, Loss: 0.225736\n",
      "Epoch: 154, Loss: 0.234149\n",
      "Epoch: 155, Loss: 0.236008\n",
      "Epoch: 156, Loss: 0.243940\n",
      "Epoch: 157, Loss: 0.237817\n",
      "Epoch: 158, Loss: 0.233204\n",
      "Epoch: 159, Loss: 0.247695\n",
      "Epoch: 160, Loss: 0.239029\n",
      "Epoch: 161, Loss: 0.243062\n",
      "Epoch: 162, Loss: 0.253877\n",
      "Epoch: 163, Loss: 0.248049\n",
      "Epoch: 164, Loss: 0.288016\n",
      "Epoch: 165, Loss: 0.285083\n",
      "Epoch: 166, Loss: 0.297028\n",
      "Epoch: 167, Loss: 0.272110\n",
      "Epoch: 168, Loss: 0.313625\n",
      "Epoch: 169, Loss: 0.294039\n",
      "Epoch: 170, Loss: 0.292558\n",
      "Epoch: 171, Loss: 0.282718\n",
      "Epoch: 172, Loss: 0.258767\n",
      "Epoch: 173, Loss: 0.259526\n",
      "Epoch: 174, Loss: 0.275982\n",
      "Epoch: 175, Loss: 0.268522\n",
      "Epoch: 176, Loss: 0.269241\n",
      "Epoch: 177, Loss: 0.269811\n",
      "Epoch: 178, Loss: 0.286882\n",
      "Epoch: 179, Loss: 0.280000\n",
      "Epoch: 180, Loss: 0.260608\n",
      "Epoch: 181, Loss: 0.261182\n",
      "Epoch: 182, Loss: 0.264012\n",
      "Epoch: 183, Loss: 0.281710\n",
      "Epoch: 184, Loss: 0.299005\n",
      "Epoch: 185, Loss: 0.311868\n",
      "Epoch: 186, Loss: 0.289746\n",
      "Epoch: 187, Loss: 0.299249\n",
      "Epoch: 188, Loss: 0.331481\n",
      "Epoch: 189, Loss: 0.357282\n",
      "Epoch: 190, Loss: 0.339118\n",
      "Epoch: 191, Loss: 0.318099\n",
      "Epoch: 192, Loss: 0.334865\n",
      "Epoch: 193, Loss: 0.298367\n",
      "Epoch: 194, Loss: 0.310431\n",
      "Epoch: 195, Loss: 0.331872\n",
      "Epoch: 196, Loss: 0.305711\n",
      "Epoch: 197, Loss: 0.314912\n",
      "Epoch: 198, Loss: 0.296648\n",
      "Epoch: 199, Loss: 0.290715\n"
     ]
    }
   ],
   "source": [
    "model.train_diagonal()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c7270f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
